{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNPm9VoGbARN1+ub3yZpXFO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhatiap70077/-Task-1-Sparks-Foundation/blob/main/Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqM3j1HaRQ7i",
        "outputId": "5f5fb2ba-3d76-470c-e918-1847013ea278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.66-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 12.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85453 sha256=f3ee47f3b064ce3ecda87a048165635b8a7cf398c69688c12d5d6a087a353569\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.1.66 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ]
        }
      ],
      "source": [
        "pip install contractions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nlpretext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TXq2vChRRsOZ",
        "outputId": "45274ba2-6d64-45a1-9b39-8e31e0c98d48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlpretext\n",
            "  Downloading nlpretext-1.1.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting dask[complete]>=2021.5.0\n",
            "  Downloading dask-2022.1.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 14.3 MB/s \n",
            "\u001b[?25hCollecting spacy>=3.0.5\n",
            "  Downloading spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 19.1 MB/s \n",
            "\u001b[?25hCollecting thinc>=8.0.4\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 48.2 MB/s \n",
            "\u001b[?25hCollecting rich>=10.1.0\n",
            "  Downloading rich-11.1.0-py3-none-any.whl (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 41.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses>=0.0.13\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 57.6 MB/s \n",
            "\u001b[?25hCollecting mosestokenizer>=1.1.0\n",
            "  Downloading mosestokenizer-1.2.1.tar.gz (37 kB)\n",
            "Collecting nltk<3.6,>=3.4.2\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 54.5 MB/s \n",
            "\u001b[?25hCollecting emoji>=0.5.2\n",
            "  Downloading emoji-1.6.3.tar.gz (174 kB)\n",
            "\u001b[K     |████████████████████████████████| 174 kB 53.7 MB/s \n",
            "\u001b[?25hCollecting flashtext>=2.7\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "Collecting pyarrow>=4.0.0\n",
            "  Downloading pyarrow-7.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.7 MB 115.9 MB/s \n",
            "\u001b[?25hCollecting pillow>=8.2.1\n",
            "  Downloading Pillow-9.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from nlpretext) (3.0.4)\n",
            "Requirement already satisfied: scikit-learn>=0.23.2 in /usr/local/lib/python3.7/dist-packages (from nlpretext) (1.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from nlpretext) (1.3.5)\n",
            "Requirement already satisfied: regex>=2019.8.19 in /usr/local/lib/python3.7/dist-packages (from nlpretext) (2019.12.20)\n",
            "Requirement already satisfied: numpy>1.15.4 in /usr/local/lib/python3.7/dist-packages (from nlpretext) (1.19.5)\n",
            "Collecting ftfy>=4.2.0\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting distributed>=2021.5.0\n",
            "  Downloading distributed-2022.1.1-py3-none-any.whl (830 kB)\n",
            "\u001b[K     |████████████████████████████████| 830 kB 45.4 MB/s \n",
            "\u001b[?25hCollecting stop-words>=2018.7.23\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "Collecting typer[all]>=0.3.2\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: importlib_metadata>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from nlpretext) (4.10.1)\n",
            "Collecting nlpaug>=1.0.1\n",
            "  Downloading nlpaug-1.1.10-py3-none-any.whl (410 kB)\n",
            "\u001b[K     |████████████████████████████████| 410 kB 61.4 MB/s \n",
            "\u001b[?25hCollecting fastparquet>=0.4.1\n",
            "  Downloading fastparquet-0.8.0-cp37-cp37m-manylinux2010_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 51.7 MB/s \n",
            "\u001b[?25hCollecting tornado>=6.0.3\n",
            "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
            "\u001b[K     |████████████████████████████████| 428 kB 53.3 MB/s \n",
            "\u001b[?25hCollecting phonenumbers>=8.10.12\n",
            "  Downloading phonenumbers-8.12.42-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 32.5 MB/s \n",
            "\u001b[?25hCollecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 55.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask[complete]>=2021.5.0->nlpretext) (0.11.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]>=2021.5.0->nlpretext) (21.3)\n",
            "Collecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Collecting pyyaml>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask[complete]>=2021.5.0->nlpretext) (1.3.0)\n",
            "Requirement already satisfied: bokeh>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from dask[complete]>=2021.5.0->nlpretext) (2.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from dask[complete]>=2021.5.0->nlpretext) (2.11.3)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.5.0->nlpretext) (2.4.0)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.5.0->nlpretext) (5.4.8)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.5.0->nlpretext) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.5.0->nlpretext) (57.4.0)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.5.0->nlpretext) (7.1.2)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.5.0->nlpretext) (1.0.3)\n",
            "Collecting cloudpickle>=1.1.1\n",
            "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.5.0->nlpretext) (1.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh>=2.1.1->dask[complete]>=2021.5.0->nlpretext) (3.10.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=2.1.1->dask[complete]>=2021.5.0->nlpretext) (2.8.2)\n",
            "Collecting cramjam>=2.3.0\n",
            "  Downloading cramjam-2.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy>=4.2.0->nlpretext) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=1.6.0->nlpretext) (3.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->dask[complete]>=2021.5.0->nlpretext) (2.0.1)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from mosestokenizer>=1.1.0->nlpretext) (0.6.2)\n",
            "Collecting openfile\n",
            "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
            "Collecting uctools\n",
            "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
            "Collecting toolwrapper\n",
            "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug>=1.0.1->nlpretext) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<3.6,>=3.4.2->nlpretext) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk<3.6,>=3.4.2->nlpretext) (4.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->dask[complete]>=2021.5.0->nlpretext) (3.0.7)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->nlpretext) (2018.9)\n",
            "Collecting locket\n",
            "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh>=2.1.1->dask[complete]>=2021.5.0->nlpretext) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug>=1.0.1->nlpretext) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug>=1.0.1->nlpretext) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug>=1.0.1->nlpretext) (2021.10.8)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.1.0->nlpretext) (2.6.1)\n",
            "Collecting colorama<0.5.0,>=0.4.0\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.2->nlpretext) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.2->nlpretext) (3.0.0)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.5->nlpretext) (2.0.6)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 16.1 MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.5->nlpretext) (3.0.6)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 63.6 MB/s \n",
            "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 44.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.5->nlpretext) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.5->nlpretext) (1.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.5->nlpretext) (0.9.0)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0.5->nlpretext) (5.2.1)\n",
            "Collecting shellingham<2.0.0,>=1.3.0\n",
            "  Downloading shellingham-1.4.0-py2.py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2021.5.0->nlpretext) (1.0.1)\n",
            "Building wheels for collected packages: emoji, flashtext, ftfy, mosestokenizer, nltk, stop-words, toolwrapper, uctools\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=bafacd45f2aadc99eb02daea3e6a246984dcafeb882f55bc97202ab2c17b0278\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9309 sha256=7deb6f1937f6b9bd1faf78e2330b57b1e43d6499650ed21c1030a1be1254ad3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/19/58/4e8fdd0009a7f89dbce3c18fff2e0d0fa201d5cdfd16f113b7\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=a5df7715f36f7c686efcd68130807730ed1682c61378d93929e298856befa6f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mosestokenizer: filename=mosestokenizer-1.2.1-py3-none-any.whl size=49189 sha256=dfe66400e855fd3e0d4ce88f9889e247bf2cfbb35ca79f58ca95a0fdbcad16cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/35/f7/af1258779a0b890abc3c79481460c597cb1f3659d0603cfb9d\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434693 sha256=d263c269833327f89bcfb6cb4e0ccc836fd62546d3b9bf11d04c57bd6d2e3036\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32911 sha256=b244b2b8484efa896489a4d77ea870fa307f0b3da796a11bdb0774a9216e8a79\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/86/b2/277b10b1ce9f73ce15059bf6975d4547cc4ec3feeb651978e9\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3353 sha256=1f6cbba5de25a87a492ec7e3a9d481815da1a886a2c67219671dc48eb521829f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/4f/33/54741ffe08e38ececb1d28068a153729b4fe820bafa0a0691f\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6161 sha256=4a5036be7dc09ba831e989ceb8525973df2d470fdb51841e6ecce33e741409bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/44/e9/914cf8fa71f0141f9314f862538d1218fcf2b94542a0fb7d35\n",
            "Successfully built emoji flashtext ftfy mosestokenizer nltk stop-words toolwrapper uctools\n",
            "Installing collected packages: locket, pyyaml, partd, fsspec, cloudpickle, catalogue, typer, tornado, srsly, pydantic, pillow, dask, uctools, toolwrapper, thinc, spacy-loggers, spacy-legacy, shellingham, pathy, openfile, langcodes, distributed, cramjam, commonmark, colorama, stop-words, spacy, sacremoses, rich, pyarrow, phonenumbers, nltk, nlpaug, mosestokenizer, ftfy, flashtext, fastparquet, emoji, nlpretext\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2.12.0\n",
            "    Uninstalling dask-2.12.0:\n",
            "      Successfully uninstalled dask-2.12.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed catalogue-2.0.6 cloudpickle-2.0.0 colorama-0.4.4 commonmark-0.9.1 cramjam-2.5.0 dask-2022.1.1 distributed-2022.1.1 emoji-1.6.3 fastparquet-0.8.0 flashtext-2.7 fsspec-2022.1.0 ftfy-6.0.3 langcodes-3.3.0 locket-0.2.1 mosestokenizer-1.2.1 nlpaug-1.1.10 nlpretext-1.1.0 nltk-3.5 openfile-0.0.7 partd-1.2.0 pathy-0.6.1 phonenumbers-8.12.42 pillow-9.0.1 pyarrow-7.0.0 pydantic-1.8.2 pyyaml-6.0 rich-11.1.0 sacremoses-0.0.47 shellingham-1.4.0 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 stop-words-2018.7.23 thinc-8.0.13 toolwrapper-2.1.0 tornado-6.1 typer-0.4.0 uctools-1.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "cloudpickle",
                  "pyarrow",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Libraries\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import contractions\n",
        "from nlpretext import Preprocessor\n",
        "from nlpretext import Preprocessor\n",
        "import nltk\n",
        "import pandas\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "text=\"\"\"This movie made it into one of my top 10 most awful movies. Horrible. \n",
        "        I don’t care if it makes 1 million, 10 M , or 100. \n",
        "        There wasn't a continuous minute where there wasn't a fight with one monster or another. \n",
        "        There was no chance for any character development, they were too busy running from one sword fight to another. \n",
        "        I had no emotional attachment ( except to the big bad machine ## that wanted to destroy them). \n",
        "        If you disagree with me, you can send your thoughts to idonotcare@leavemealone.com\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGe8R6udRgH5",
        "outputId": "1f971c77-e23e-48cb-9d64-c430d5d1bf7a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Text_Preprocess:\n",
        "    def exp_contractions(self,text:str)->str:\n",
        "            # creating an empty list\n",
        "        expanded_words = []    \n",
        "        for word in text.split():\n",
        "            # using contractions.fix to expand the shotened words\n",
        "            expanded_words.append(contractions.fix(word))   \n",
        "\n",
        "        expanded_text = ' '.join(expanded_words)\n",
        "        return expanded_text\n",
        "    \n",
        "    def spec_char(self,text:str)->str:\n",
        "        preprocessor = Preprocessor()\n",
        "        Formatted_Text = re.sub(r\"[^A-Za-z]+\", ' ', text)\n",
        "        spec_text = preprocessor.run(Formatted_Text)\n",
        "        return spec_text\n",
        "    \n",
        "    def simp_dig_email(self,text:str)->str:\n",
        "        preprocessor = Preprocessor()\n",
        "        Formatted_Text = re.sub(r\"-@]+\", ' ', text)\n",
        "        simp_text = preprocessor.run(Formatted_Text)\n",
        "        return simp_text\n",
        "    \n",
        "    def tokenization(self,text:str)->list:\n",
        "        tokenized_text = nltk.tokenize.word_tokenize(text)\n",
        "        return tokenized_text\n",
        "    \n",
        "    def stopwords(self,tokens:list, method:str)->list:\n",
        "        nltk.download('stopwords')\n",
        "        list_of_stop_words = stopwords.words('english')\n",
        "\n",
        "        list_of_stop_words[1:10]\n",
        "\n",
        "        token_without_stopwords = [i for i in tokens if i not in list_of_stop_words]\n",
        "        return token_without_stopwords\n",
        "    \n",
        "    \n",
        "   \n",
        "    \n",
        "    def get_preprocessed_text(self):\n",
        "        con_text = self.exp_contractions(text)\n",
        "        special_text = self.spec_char(con_text)\n",
        "        simplify_text = self.simp_dig_email(special_text)\n",
        "        token_text = self.tokenization(simplify_text)\n",
        "        stopword_text = self.stopwords(token_text, 'english')\n",
        "        return stopword_text\n",
        "        \n",
        "#Class Object\n",
        "Class_obj = Text_Preprocess()\n",
        "Class_obj.get_preprocessed_text()    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1nNtSvUSG-p",
        "outputId": "385bfb0a-5d3e-4e13-bc3b-334e3e293176"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'movie',\n",
              " 'made',\n",
              " 'one',\n",
              " 'top',\n",
              " 'awful',\n",
              " 'movies',\n",
              " 'Horrible',\n",
              " 'I',\n",
              " 'care',\n",
              " 'makes',\n",
              " 'million',\n",
              " 'M',\n",
              " 'There',\n",
              " 'continuous',\n",
              " 'minute',\n",
              " 'fight',\n",
              " 'one',\n",
              " 'monster',\n",
              " 'another',\n",
              " 'There',\n",
              " 'chance',\n",
              " 'character',\n",
              " 'development',\n",
              " 'busy',\n",
              " 'running',\n",
              " 'one',\n",
              " 'sword',\n",
              " 'fight',\n",
              " 'another',\n",
              " 'I',\n",
              " 'emotional',\n",
              " 'attachment',\n",
              " 'except',\n",
              " 'big',\n",
              " 'bad',\n",
              " 'machine',\n",
              " 'wanted',\n",
              " 'destroy',\n",
              " 'If',\n",
              " 'disagree',\n",
              " 'send',\n",
              " 'thoughts',\n",
              " 'idonotcare',\n",
              " 'leavemealone',\n",
              " 'com']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NdqLOCUUSR4w"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}